spring:
  application:
    name: learning-ai

  ai:
    # For Ollama (local model)
    ollama:
      base-url: http://localhost:11434   # where your Ollama server runs        # or your desired strategy
      chat:
        options:
          model: deepseek-r1:8b                # or whichever local model you pulled
          temperature: 0.7
          # other Ollama-specific options if needed

    # For OpenAI / Perplexity-compatible API
    openai:
      api-key: ${PERPLEXITY_API_KEY}
      base-url: "https://api.perplexity.ai"
      chat:
        completions-path: "/chat/completions"
        options :
          model : "sonar"
    chat:
      client:
        enabled: false

logging:
  pattern:
    console : "%green(%d{HH:mm:ss.SSS}) %blue(%-5level) %red([%thread]) %yellow(%logger{15}) - %msg%n"
  level:
    org:
      springframework:
        ai:
          chat:
            client:
              advisor: DEBUG